/**
 * LLM Cost Calculation Utilities.
 *
 * This file contains functions for calculating the costs associated with using large language models (LLMs) based on
 * token usage and model-specific pricing. It helps in budgeting and monitoring the financial aspects of using LLMs within
 * the AgenticJS library.
 *
 * Usage:
 * Apply these functions to compute and track costs as part of operational management and optimization of LLM usage in projects.
 */

import { logger } from "./logger";
import { IUsageStats } from "./types";

const MODELS_PRICING = [
  // GPT Models from OpenAI
  {
    modelCode: "gpt-4o-mini",
    provider: "openai",
    inputPricePerMillionTokens: 0.15,
    outputPricePerMillionTokens: 0.6,
    features: "Most cost-efficient with vision capabilities",
  },
  {
    modelCode: "gpt-3.5-turbo",
    provider: "openai",
    inputPricePerMillionTokens: 0.5,
    outputPricePerMillionTokens: 1.5,
    features: "Cost-effective option",
  },
  {
    modelCode: "gpt-3.5-turbo-0125",
    provider: "openai",
    inputPricePerMillionTokens: 0.5,
    outputPricePerMillionTokens: 1.5,
    features: "Cost-effective option",
  },
  {
    modelCode: "gpt-4o",
    provider: "openai",
    inputPricePerMillionTokens: 5.0,
    outputPricePerMillionTokens: 15.0,
    features: "Enhanced multimodal capabilities",
  },
  {
    modelCode: "gpt-4-turbo",
    provider: "openai",
    inputPricePerMillionTokens: 10.0,
    outputPricePerMillionTokens: 30.0,
    features: "High speed and power",
  },
  {
    modelCode: "gpt-4",
    provider: "openai",
    inputPricePerMillionTokens: 30.0,
    outputPricePerMillionTokens: 60.0,
    features: "Latest generation AI",
  },
  // Claude Models from Anthropic
  {
    modelCode: "claude-3-5-sonnet-20240620",
    provider: "anthropic",
    inputPricePerMillionTokens: 3,
    outputPricePerMillionTokens: 15,
    features: "Highest level of intelligence and capability",
  },
  {
    modelCode: "claude-3-opus-20240229",
    provider: "anthropic",
    inputPricePerMillionTokens: 15,
    outputPricePerMillionTokens: 75,
    features: "Top-level performance, intelligence, fluency, and understanding",
  },
  {
    modelCode: "claude-3-sonnet-20240229",
    provider: "anthropic",
    inputPricePerMillionTokens: 3,
    outputPricePerMillionTokens: 15,
    features: "Strong utility, balanced for scaled deployments",
  },
  {
    modelCode: "claude-3-haiku-20240307",
    provider: "anthropic",
    inputPricePerMillionTokens: 0.25,
    outputPricePerMillionTokens: 1.25,
    features: "Quick and accurate targeted performance",
  },
  // Gemini Models from Google
  {
    modelCode: "gemini-1.5-flash",
    provider: "google",
    inputPricePerMillionTokens: 0.35,
    outputPricePerMillionTokens: 1.05,
    features: "Fast multimodal model with 1 million token context window",
  },
  {
    modelCode: "gemini-1.5-pro",
    provider: "google",
    inputPricePerMillionTokens: 3.5,
    outputPricePerMillionTokens: 10.5,
    features: "Next-generation model with 2 million token context window",
  },
  {
    modelCode: "gemini-1.0-pro",
    provider: "google",
    inputPricePerMillionTokens: 0.5,
    outputPricePerMillionTokens: 1.5,
    features: "First-generation model, only text and image reasoning",
  },
  // Mistral Models
  {
    modelCode: "open-mistral-nemo-2407",
    provider: "mistral",
    inputPricePerMillionTokens: 0.3,
    outputPricePerMillionTokens: 0.3,
    features:
      "Mistral Nemo is a state-of-the-art 12B model developed with NVIDIA",
  },
  {
    modelCode: "mistral-large-2407",
    provider: "mistral",
    inputPricePerMillionTokens: 3,
    outputPricePerMillionTokens: 9,
    features:
      "Top-tier reasoning for high-complexity tasks, for your most sophisticated needs",
  },
  {
    modelCode: "codestral-2405",
    provider: "mistral",
    inputPricePerMillionTokens: 1,
    outputPricePerMillionTokens: 3,
    features:
      "State-of-the-art Mistral model trained specifically for code tasks",
  },
];

export interface ICostDetails {
  costInputTokens: number;
  costOutputTokens: number;
  totalCost: number;
}

/**
 * Calculates the approximate cost of using a specified AI model based on the number of input and output tokens.
 * If the model code does not match any in the predefined list, it returns a standardized error response with costs set to -1.
 *
 * @param {string} modelCode - The unique code identifier for the AI model.
 * @param {object} llmUsageStats - An object containing usage statistics including:
 *     - inputTokens: The number of input tokens processed.
 *     - outputTokens: The number of output tokens generated.
 *     - callsCount: The total number of calls made.
 *     - callsErrorCount: The number of failed calls.
 *     - parsingErrors: The number of errors encountered during parsing.
 * @returns {object} An object containing:
 *     - costInputTokens: The calculated cost for the input tokens or -1 if the model is not found.
 *     - costOutputTokens: The calculated cost for the output tokens or -1 if the model is not found.
 *     - totalCost: The total cost combining input and output tokens or -1 if the model is not found.
 *
 * @example
 * // Calculate costs for a known model with usage stats
 * const llmUsageStats = {
 *   inputTokens: 500000,
 *   outputTokens: 200000,
 *   callsCount: 10,
 *   callsErrorCount: 1,
 *   parsingErrors: 0
 * };
 * const costDetails = calculateCost('gpt-4o-mini', llmUsageStats);
 * console.log(costDetails);
 */
const calculateTaskCost = (
  modelCode: string,
  llmUsageStats: IUsageStats
): ICostDetails => {
  const model = MODELS_PRICING.find((m) => m.modelCode === modelCode);
  if (!model) {
    return {
      costInputTokens: -1,
      costOutputTokens: -1,
      totalCost: -1,
    };
  }

  const inputCost =
    (llmUsageStats.inputTokens / 1000000) * model.inputPricePerMillionTokens;
  const outputCost =
    (llmUsageStats.outputTokens / 1000000) * model.outputPricePerMillionTokens;
  const totalCost = inputCost + outputCost;

  return {
    costInputTokens: parseFloat(inputCost.toFixed(4)),
    costOutputTokens: parseFloat(outputCost.toFixed(4)),
    totalCost: parseFloat(totalCost.toFixed(4)),
  };
};

/**
 * Calculates the total cost of using multiple AI models in a workflow based on their respective usage statistics.
 */
const calculateTotalWorkflowCost = (
  modelUsageStats: Record<string, IUsageStats>
): ICostDetails => {
  let totalInputCost = 0;
  let totalOutputCost = 0;
  let totalCost = 0;
  let totalInputTokens = 0;
  let totalOutputTokens = 0;
  let allPricesAvailable = true;

  Object.keys(modelUsageStats).forEach((modelCode) => {
    const stats = modelUsageStats[modelCode];
    const model = MODELS_PRICING.find((m) => m.modelCode === modelCode);
    if (!model) {
      logger.warn(`No pricing information found for model ${modelCode}`);
      allPricesAvailable = false;
      return;
    }
    const inputCost =
      (stats.inputTokens / 1000000) * model.inputPricePerMillionTokens;
    const outputCost =
      (stats.outputTokens / 1000000) * model.outputPricePerMillionTokens;
    totalInputCost += inputCost;
    totalOutputCost += outputCost;
    totalCost += inputCost + outputCost;
    totalInputTokens += stats.inputTokens;
    totalOutputTokens += stats.outputTokens;
  });

  if (!allPricesAvailable) {
    return {
      costInputTokens: -1,
      costOutputTokens: -1,
      totalCost: -1,
    };
  }

  return {
    costInputTokens: parseFloat(totalInputCost.toFixed(4)),
    costOutputTokens: parseFloat(totalOutputCost.toFixed(4)),
    totalCost: parseFloat(totalCost.toFixed(4)),
  };
};

export { calculateTaskCost, calculateTotalWorkflowCost };
